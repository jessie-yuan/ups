<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering">
  <meta name="keywords" content="UPS">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Uncertainty-Aware Policy Steering</title>
  <link rel="shortcut icon" href="./static/images/robot.png">
  
  <script>
	window.dataLayer = window.dataLayer || [];

	function gtag() {
	  dataLayer.push(arguments);
	}

	gtag('js', new Date());

	gtag('config', 'G-PYVRSFMDRL');
	
	// Change Video
	  
	const hardwareObjectList = [
		"./static/videos/hardware_results/straightforward_hardware",
		"./static/videos/hardware_results/ambiguous_hardware",
		"./static/videos/hardware_results/incapable_hardware"
	];
	
	const simObjectList = [
		"./static/videos/sim_results/straightforward_sim",
		"./static/videos/sim_results/ambiguous_sim",
		"./static/videos/sim_results/incapable_sim"
	]
	  
	const failureTextList = [
		"In this example, the actual execution of the action plan knocks the cup down on the table while the world model mistakenly imagines the robot successfully picks up the cup from the table via the handle.",
		"In this example, the actual execution matches the world model's imagination as the robot picks up the chip bag via the edge but the VLM generates a wrong behavior narration, saying that the robot fails to pick up the bag.",
		"In this example, the behavior narrations generated by the VLM are accurate, but the VLM selects the wrong action plan with the wrong reasoning given the task description."
	];
	  
	function switchHardwareVideo() {
		var object = document.hardwareObjectForm.switch.options[document.hardwareObjectForm.switch.selectedIndex].value;
		var videoList = Array();
		
//		document.getElementById("test_text").textContent= "abc";
		document.getElementById("hardware_video").src = hardwareObjectList[object] + ".mp4";
//		document.getElementById("real_plot_success").src = successObjectList[object] + ".html";
//		document.getElementById("test_text").textContent= successObjectList[object];
	}
	  
	function switchSimVideo() {
		var object = document.simObjectForm.switch.options[document.simObjectForm.switch.selectedIndex].value;
		var videoList = Array();
		
		document.getElementById("sim_video").src = simObjectList[object] + ".mp4";
//		document.getElementById("real_plot_failure").src = failureObjectList[object] + ".html";
		// document.getElementById("failure-text").textContent= failureTextList[object];
	}
	
	 
	 
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css?v=2">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="./index.html">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">When to Act, Ask, or Learn: <br>Uncertainty-Aware Policy Steering</h1>
          <h1 class="title is-5 publication-title">Under Review</h1>
		  <div class="is-size-5 publication-authors">
			<span class="author-block">
				<a href="https://jessie-yuan.github.io/">Jessie Yuan</a><sup>1*</sup>,</span>
			<span class="author-block">
				<a href="https://yilin-wu98.github.io/">Yilin Wu</a><sup>1*</sup>,</span>
				<span class="author-block">
				<a href="https://www.cs.cmu.edu/~abajcsy/">Andrea Bajcsy</a><sup>1</sup>
				</span>
			
          </div>

          <div class="is-size-10 publication-authors">
            <span class="author-block"><sup>1</sup>Carnegie Mellon University,      *denotes equal contribution</span>
          </div>

		  <div class="column has-text-centered">
            <div class="publication-links">
				<span class="link-block">
					<a href="https://jessie-yuan.github.io/ups/index.html"
						class="external-link button is-normal is-rounded is-dark">
						<span class="icon">
							<i class="ai ai-arxiv"></i>
						</span>
						<span>arXiv</span>
					</a>
					</span>
				<!-- <span class="link-block">
				<a href="https://github.com/CMU-IntentLab/Forewarn/tree/main"
					class="external-link button is-normal is-rounded is-dark">
					<span class="icon">
						<i class="fab fa-github"></i>
					</span>
					<span>Code</span>
					</a>
				</span> -->
            </div>
		Code coming soon!

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <video id="teaser" controls autoplay muted loop playsinline width="70%">
        <source src="./static/videos/overall_vid.mp4"
                type="video/mp4">
      </video>
		<br>
      <p class="has-text-centered">
		<br>
        Our method <span class="dnerf">UPS</span> addresses VLM verifier overconfidence in policy steering by quantifying its uncertainty to distinguish semantic task uncertainty from low-level action feasibility and select an appropriate uncertainty resolution strategy: executing a high-confidence action, asking a clarifying question about the task, or requesting demonstrations to retrain the low-level policy.
      </p>
    </div>
  </div>
</section>
		  
		  
		  
		  
			
			
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/opening_carousel/base_straightforward.mp4"
                    type="video/mp4">
          </video>
		  <p>
			<strong>Method:</strong> <span style="color: #959595">Base policy alone</span><br>
			<strong>Scenario category:</strong> Straightforward<br>
			<strong>User instruction:</strong> "On the left side, there is a bin -- place the cup there." <br>
		  	<strong>Outcome:</strong> <span style="color: red">Failure</span>
		  </p>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/opening_carousel/ups_straightforward.mp4"
                    type="video/mp4">
          </video>
		  <p>
			<strong>Method:</strong> <span style="color: #f16a14">UPS</span><br>
			<strong>Scenario category:</strong> Straightforward<br>
			<strong>User instruction:</strong> "The right side of the table has a bin -- can you put the cup there?"<br>
		  	<strong>Outcome:</strong> <span style="color: green">Success</span>
		  </p>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/opening_carousel/base_ambiguous.mp4"
                    type="video/mp4">
          </video>
		  <p>
			<strong>Method:</strong> <span style="color: #959595">Base policy alone</span><br>
			<strong>Scenario category:</strong> Ambiguous (intention: left)<br>
			<strong>User instruction:</strong> "Put the nut on the peg so that its handle faces my dominant-hand side." <br>
		  	<strong>Outcome:</strong> <span style="color: red">Failure</span>
		  </p>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/opening_carousel/ups_ambiguous_paused.mp4"
                    type="video/mp4">
          </video>
		  <p>
			<strong>Method:</strong> <span style="color: #f16a14">UPS</span><br>
			<strong>Scenario category:</strong> Ambiguous (intention: left)<br>
			<strong>User instruction:</strong> "Put the nut on the peg so that its handle faces my dominant-hand side." <br>
		  	<strong>Outcome:</strong> <span style="color: green">Success</span>
		  </p>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/opening_carousel/base_incapable.mp4"
                    type="video/mp4">
          </video>
		  <p>
			<strong>Method:</strong> <span style="color: #959595">Base policy alone</span><br>
			<strong>Scenario category:</strong> Incapable <br>
			<strong>User instruction:</strong> "Could you move the cup to the bin on the right side?" <br>
		  	<strong>Outcome:</strong> <span style="color: red">Failure</span>
		  </p>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/opening_carousel/ups_incapable.mp4"
                    type="video/mp4">
          </video>
          <p>
			<strong>Method:</strong> <span style="color: #f16a14">UPS</span><br>
			<strong>Scenario category:</strong> Incapable <br>
			<strong>User instruction:</strong> "Ensure the cup is inside the right bin." <br>
		  	<strong>Outcome:</strong> <span style="color: green">Success</span>
		  </p>
        </div>
      </div>
    </div>
  </div>
</section>
  

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          	<p>
				Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. 
				<br><br>
				We propose <em>uncertainty-aware policy steering</em> (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
	  	<h2 class="title is-3"><span class="dnerf">UPS: Uncertainty-aware Policy Steering</span></h2>
	  	<br>
		<img src="static/figures/method.png" alt="Method Figure">
		<br><br>
		<p>
			Our framework calibrates the VLM verifier used for policy steering via conformal
prediction. This enables the VLM to select an appropriate way to resolve uncertainty, from querying the end-user in natural
language to asking to re-train the low-level control policy.
		</p>
	</div>
		
	</div>
  </div>
</section>

<section class="section">
	<div class="container is-max-desktop">
	  <!-- Animation. -->
	  <div class="columns is-centered">
		<div class="column is-full-width">
			<h2 class="title is-3">Outcome Prediction and Narration</h2>
			<br>
		  <img src="static/figures/interleaving.png" alt="interleaving figure">
		  <br><br>
		  <p>
			Building on prior work, we use a world model to predict future observations of any action sample and then narrate the predictions in natural language, enabling zero-shot VLM verification.
			However, most low-level action policies generate short-horizon action chunks. Thus, all action samples result in similar future observations and indistinguishable narrations.
			Therefore, we <em>interleave</em> action generation and world model imaginations to create longer-horizon action sequences, and use all the decoded images for narration.
		  </p>
	  </div>
		  
	  </div>
	</div>
  </section>

  
  <!--real robot results against the baslines-->
<!-- ======================= -->
<!-- Real World Results Section -->
<!-- ======================= -->

<section class="section" id="real-world-results">
	<div class="container is-fluid">
	  <!-- Section Title -->
	 <div class="container is-max-desktop">
	  <div class="columns">
		<div class="column is-four-fifths">
		  <h2 class="title is-3">Simulation and Real World Results</h2>
		</div>
		</div>
	  </div>
	  <div class="container is-max-desktop">
	  <div class ="columns ">
		<div class="column is-full-width" style="margin-top: 20px;">
			<h3 class="title is-4">Quantitative Results for Conformal Prediction</h3>
			<!-- <p>We demonstrate real-world deployments of our system across various tasks.</p> -->
			 <p><span style="font-weight: bold;font-size: 20px"> Score Functions </span><br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#fd994e"> Bayesian Intent (Ours) </span> : First queries the VLM to hypothesize a set of potential human intents and asks the VLM to score each of these potential intents based on the user's instruction. 
				Then, given a specific intent, query the VLM to estimate the likelihood of each behavior narration and marginalize over these quantities.<br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#85cc84"> Chain-of-Thought Reasoning (CoT) </span>: First asks the VLM to reason about the instruction, then generate the probabilities for each behavior narration conditioned on this reasoning. <br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#7fb8da"> Vanilla </span>: Asks the VLM to directly self-generate the probabilities for each behavior narration. <br>
				<br>
				<span style="font-weight: bold;font-size: 20px"> Uncertainty Quantification Approaches </span><br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#bfbfbf"> SimpleSet </span>: Sorts the options from high probability to low and adds them to the prediction set until their sum exceeds the uncalibrated threshold 1 - epsilon.<br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#8c8c8d"> Adaptive Prediction Set (APS) </span>: Sorts the options from high probability to low and adds them to the prediction set until their sum exceeds a calibrated threshold, 1 - qhat.<br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#606061">Conformal Prediction (CP) </span>: Forms a prediction set by comparing the probabilities of each individual option with a calibrated threshold, 1 - qhat.<br>
				<br>
				<!-- The coverage rate is the proportion of test samples whose ground-truth option(s) are included in the prediction set. 
				The clarification rate is the proportion of samples in which the prediction set is non-singleton and thus the robot asks the human a question. 
				The set size is the size of the prediction set. -->
				Across both simulation and hardware, when paired with CP, our intent-aware score function achieves higher coverage than baselines in ambiguous and incapable cases. It also selectively seeks clarifications for ambiguous scenarios while avoiding unnecessary requests in straightforward or incapable cases.
				 <!-- For instance, in the Bag task, <span id="text" style="font-weight: bold;font-size: 15px;color:#26A6D5">VLM-Act </span>, 
				<span id="text" style="font-weight: bold;font-size: 15px;color:#0d79ca"> VLM-Img </span>, 
				and <span id="text" style="font-weight: bold;font-size: 15px;color:#233c80"> VLM-Img-Oracle </span>  all hallucinate that the robot is grasping the edge of the
				bag, whereas it is actually grasping the middle. -->
			</p>
		  <!-- <p> -->
		<!-- </p> -->
		 </div>
		</div>
		</div>
		<!-- <section class="section"> -->
			<div class="container is-max-desktop">
			  <!-- Two columns -->
			   <!-- Right column (video) - smaller width -->
			   <div class ="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<!-- <h4 class="title is-5">Cup Task</h4> -->

					<!-- <p>We demonstrate real-world deployments of our system across various tasks.</p> -->
				  <!-- <p> -->
				<!-- </p> -->
				 </div>
				</div>
			   <div class="columns">
				
				<div class="column is-one-half has-text-centered">
				<!-- <video
				style="width: 60%;"
				autoplay
				loop
				muted
				playsinlines
				controls
			  > -->
			   <!-- put image /Users/jessieyuan/Documents/ups/static/images/simulation_uq_plot_color.pdf here -->
				<img
					  src="static/figures/hardware_uq_plot.png"
					  alt="Right image"
					  style="width: 85%;"
					/>
				<!-- <source src="static/videos/cup_wrist_bn.mp4" type="video/mp4"> -->
				<!-- Fallback text if video unsupported -->
			  <!-- </video> -->
			</div>
		

				
				
				  <!-- Left column (image) - larger width -->
				<div class="column is-one-half has-text-centered">
					<img
					  src="static/figures/sim_uq_plot.png"
					  alt="Left image"
					  style="width: 85%;"
					/>
			
				</div>
			</div>
			</div>
		  <!-- </section> -->
		  
		<div class="container is-max-desktop">
			
		<div class ="columns">
			<div class="column is-full-width" style="margin-top: 100px;">
				<h3 class="title is-4">Quantitative Results for Continual Learning</h3>
				<div class="container is-max-desktop">
				<p><span style="font-weight: bold;font-size: 20px"> Baselines </span><br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#959595"> Base Policy </span> : Unaltered base diffusion policy.<br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#b6b6d8"> Human-Gated (HG) DAgger + Residual </span>: Base diffusion policy + residual policy trained on human-gated interventions.<br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#8682bc"> EnsembleDAgger </span>: Base diffusion policy + residual policy trained on interventions collected by querying human demonstrator whenever an ensemble of diffusion policies disagrees.<br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#fdcfa1"> FOREWARN </span>: UPS with base diffusion policy alone and without uncertainty quantification; queries the VLM verifier to select the best behavior narration without conformal prediction or asking clarifying questions.<br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#fd994e"> UPS w/ Clarification </span>: UPS with base diffusion policy alone.<br>
				<span id="text" style="font-weight: bold;font-size: 18px;color:#f16a14"> UPS w/ Clarification + Residual (Ours) </span>: UPS with base diffusion policy + residual policy trained from prior incapable scenarios.<br>
				<br>
				<!-- The coverage rate is the proportion of test samples whose ground-truth option(s) are included in the prediction set. 
				The clarification rate is the proportion of samples in which the prediction set is non-singleton and thus the robot asks the human a question. 
				The set size is the size of the prediction set. -->
				By calibrating uncertainty and strategically asking for clarifications, our approach improves success rate over the base policy in ambiguous cases. Additionally, by combining our uncertainty quantification approach with three targeted resolution strategies (execute, clarify, re-train), our approach improves performance after residual training.
				 <!-- For instance, in the Bag task, <span id="text" style="font-weight: bold;font-size: 15px;color:#26A6D5">VLM-Act </span>, 
				<span id="text" style="font-weight: bold;font-size: 15px;color:#0d79ca"> VLM-Img </span>, 
				and <span id="text" style="font-weight: bold;font-size: 15px;color:#233c80"> VLM-Img-Oracle </span>  all hallucinate that the robot is grasping the edge of the
				bag, whereas it is actually grasping the middle. -->
			</p>
			<br><br><br>
	</div>
	<div class ="columns is-centered has-text-centered">
		<img src="static/figures/success_rate.png" alt="behavior narration" style="width: 70%;"> 
	</div>
				<!-- <p>We demonstrate real-world deployments of our system across various tasks.</p> -->
			  <!-- <p> -->
			<!-- </p> -->
			 </div>
			</div>
			</div>



</div>
</div>
<br><br><br>
<div class="container is-max-desktop">
	  <div class ="columns ">
		<div class="column is-full-width" style="margin-top: 20px;">
			<h3 class="title is-4">Qualitative Results</h3>
			<!-- <p>We demonstrate real-world deployments of our system across various tasks.</p> -->
		  <!-- <p> -->
		<!-- </p> -->
		 </div>
		</div>
		</div>
		<!-- <section class="section"> -->
			<div class="container is-max-desktop">
			  <!-- Two columns -->
			   <!-- Right column (video) - smaller width -->
			   <div class ="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<!-- <h4 class="title is-5">Cup Task</h4> -->

					<!-- <p>We demonstrate real-world deployments of our system across various tasks.</p> -->
				  <!-- <p> -->
				<!-- </p> -->
				 </div>
				</div>
			   <div class="columns">
				
			<div class="column is-one-half has-text-centered">
				<!-- <video
				style="width: 60%;"
				autoplay
				loop
				muted
				playsinlines
				controls
			  > -->
			   <!-- put image /Users/jessieyuan/Documents/ups/static/images/simulation_uq_plot_color.pdf here -->
			<p>
			Scenario Category - Hardware: &nbsp; </p>
			<form method="" action="" name="hardwareObjectForm">
			<select size="1" name="switch" onchange="switchHardwareVideo();">
				<option value="0">Straightforward</option>
				<option value="1">Ambiguous</option>
				<option value="2">Incapable</option>
			</select>
			</form>
			<br>

			<video id="hardware_video" controls autoplay muted loop playsinline width="85%">
				<source src="./static/videos/hardware_results/straightforward_hardware.mp4"
						type="video/mp4">
			</video>
				<!-- Fallback text if video unsupported -->
			  <!-- </video> -->
			</div>
		

				
				
				  <!-- Left column (image) - larger width -->
				<div class="column is-one-half has-text-centered">
				<!-- <video
				style="width: 60%;"
				autoplay
				loop
				muted
				playsinlines
				controls
			  > -->
			   <!-- put image /Users/jessieyuan/Documents/ups/static/images/simulation_uq_plot_color.pdf here -->
			<p>
			Scenario Category - Simulation: &nbsp; </p>
			<form method="" action="" name="simObjectForm">
			<select size="1" name="switch" onchange="switchSimVideo();">
				<option value="0">Straightforward</option>
				<option value="1">Ambiguous</option>
				<option value="2">Incapable</option>
			</select>
			</form>
			<br>

			<video id="sim_video" controls autoplay muted loop playsinline width="85%">
				<source src="./static/videos/sim_results/straightforward_sim.mp4"
						type="video/mp4">
			</video>
				<!-- Fallback text if video unsupported -->
			  <!-- </video> -->
			</div>
			</div>
			</div>
		  <!-- </section> -->
		  
	
  </section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> # TODO

  
</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
<!--
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
-->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p class="has-text-centered">
            Website template adapted from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
			</p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>


